{
  "stage": "COMPLETED",
  "run_qa": true,
  "run_improver": true,
  "jd": {
    "role_title": "Senior AI Engineer",
    "company": "Sparq",
    "seniority_level": "Senior",
    "must_have_skills": [
      "AI/ML model development",
      "Model training and fine-tuning",
      "Python",
      "Data preprocessing",
      "Cloud deployment of AI solutions",
      "Scalability and performance optimization",
      "Monitoring and optimization of models in production (MLOps)",
      "Automation of AI workflows",
      "Collaboration with software development teams",
      "Research and evaluation of new AI technologies",
      "Consultative problem solving",
      "Project oversight and delivery",
      "Leadership and mentoring",
      "Software development life cycle (SDLC)",
      "Communication skills"
    ],
    "nice_to_have_skills": [
      "AWS, Azure, or GCP experience",
      "MLOps tooling (e.g., MLflow, SageMaker, Kubeflow)",
      "Containerization and orchestration (Docker, Kubernetes)",
      "CI/CD for ML",
      "Agile methodologies",
      "Presentation and coaching experience",
      "Industry-spanning project experience",
      "AI/ML or cloud certifications"
    ],
    "notes_for_resume": "- Highlight end-to-end AI model design, training/fine-tuning, and successful production deployments.\n- Emphasize Python expertise for data preprocessing and automation of ML workflows.\n- Showcase cloud implementations (AWS/Azure/GCP), including scalability/performance tuning and production monitoring.\n- Quantify outcomes (e.g., accuracy gains, latency reduction, cost savings, reliability/uptime).\n- Demonstrate consultative client engagement aligning AI solutions to business goals.\n- Include collaboration with product/dev teams and examples of leading/mentoring engineers.\n- Note SDLC mastery, agile delivery, and project oversight responsibilities.\n- List relevant MLOps tools, CI/CD pipelines, and container/Kubernetes experience if applicable.\n- Add certifications and evidence of continuous learning/research in emerging AI technologies.\n- State U.S. work authorization and comfort with remote/distributed teams."
  },
  "profile": {
    "full_name": "NARENDER SURABHI",
    "headline": "SENIOR MACHINE LEARNING ENGINEER | GENERATIVE AI & MLOPS ARCHITECT",
    "location": "Okemos, MI, USA 48864",
    "phone": "+1 (213) 254-8205",
    "email": "surabhinarenderrao@gmail.com",
    "linkedin_url": "https://www.linkedin.com/in/narendersurabhi",
    "github_url": "https://github.com/narendersurabhi",
    "years_of_experience": 17.25,
    "core_skills": [
      "Generative AI",
      "Large Language Models (LLMs)",
      "RAG",
      "Agentic AI",
      "Machine Learning",
      "Risk Analytics",
      "MLOps",
      "Data Engineering",
      "Cloud Architecture",
      "Model Deployment",
      "Monitoring & Observability",
      "Python",
      "SQL"
    ],
    "domain_expertise": [
      "Medicaid/Medicare",
      "Healthcare claims",
      "FWA analytics",
      "Provider network integrity",
      "Care management & outreach",
      "HIPAA",
      "EDI X12 5010",
      "ICD-10",
      "CPT",
      "HCPCS",
      "Membership & benefits",
      "SURS audits"
    ],
    "tools_and_tech": [
      "AWS SageMaker",
      "AWS Bedrock",
      "AWS Glue",
      "Amazon Redshift",
      "Amazon S3",
      "AWS Lambda",
      "AWS Step Functions",
      "SageMaker Clarify",
      "MLflow",
      "Docker",
      "Kubernetes/EKS",
      "Spark",
      "PySpark",
      "Kafka",
      "Python",
      "Pandas",
      "NumPy",
      "SciPy",
      "Scikit-learn",
      "PyTorch",
      "TensorFlow/Keras",
      "XGBoost",
      "LangChain",
      "LangGraph",
      "FAISS",
      "Chroma",
      "PEFT",
      "LoRA",
      "SQL",
      "Java",
      "REST APIs",
      "Git",
      "Bitbucket",
      "Jenkins",
      "EC2",
      "CloudFormation"
    ],
    "experience": [
      {
        "title": "Senior Machine Learning Engineer & AI Solutions Architect",
        "company": "Acentra Health",
        "start_date": "Dec 2016",
        "end_date": "Present",
        "location": "Okemos, MI",
        "bullets": [
          "AI/ML lead for Medicaid integrity and care-management products, owning architecture, implementation, and productionization of ML and GenAI capabilities.",
          "Designed and deployed LLM-powered RAG features on AWS Bedrock to auto-generate fraud, provider risk, and patient risk narratives, reducing reviewer time.",
          "Built agentic AI workflows (LangGraph + Bedrock) for multi-step fraud pattern discovery, data retrieval, and explanation generation.",
          "Engineered HIPAA-compliant ML pipelines on SageMaker and Glue (PySpark) processing 116M+ Medicaid claims annually with MLflow-based experiment tracking and governed deployments.",
          "Operationalized supervised and unsupervised models for pre-pay and post-pay FWA detection, provider risk scoring, and patient risk stratification.",
          "Implemented drift, bias, and performance monitoring using SageMaker Clarify and custom dashboards; defined SLOs and alerts.",
          "Designed robust APIs and data contracts to expose scores, risk tiers, and GenAI explanations to downstream systems.",
          "Mentored engineers and analysts on GenAI, MLOps, Python best practices, and experimentation; influenced AI roadmap, standards, and governance.",
          "Medicaid Patient Risk Scoring: engineered features from claims, utilization, chronic conditions, demographics, and gaps in care; deployed on SageMaker with Glue/Redshift/S3; delivered ranked lists and explanations.",
          "Medicaid Provider Risk Scoring: built provider risk engine aggregating claim, member, and peer-comparison features; integrated into Program Intelligence and SIU workflows.",
          "Program Intelligence: combined supervised models, unsupervised pattern mining, network analysis, and LLM-based narratives to detect fraud, waste, and abuse.",
          "AuditStudio (North Dakota): developed unsupervised PCA/KMeans pipelines and Glue PySpark ETL with Redshift/S3 to prioritize audits.",
          "ClaimsSure: delivered pre-pay fraud scoring (logistic regression + rule/scenario logic) with production APIs, thresholds/overrides, and latency/accuracy monitoring."
        ],
        "skills": [
          "AWS Bedrock",
          "AWS SageMaker",
          "AWS Glue",
          "Amazon Redshift",
          "Amazon S3",
          "AWS Lambda",
          "AWS Step Functions",
          "MLflow",
          "LangChain",
          "LangGraph",
          "FAISS",
          "Chroma",
          "Python",
          "PySpark",
          "Scikit-learn",
          "PyTorch",
          "TensorFlow",
          "XGBoost",
          "Logistic Regression",
          "PCA",
          "KMeans",
          "SageMaker Clarify",
          "SQL",
          "Docker"
        ]
      },
      {
        "title": "Senior Associate Consultant (Software Engineer)",
        "company": "Infosys Limited",
        "start_date": "Sep 2013",
        "end_date": "Dec 2015",
        "location": "Los Angeles, CA & Pune, India",
        "bullets": [
          "Developed and enhanced modules for Medicaid encounter processing and Open Health Plus using Java, PL/SQL, and SQL.",
          "Implemented backend services, stored procedures, and batch jobs to process high-volume healthcare claims and membership data.",
          "Refactored legacy code into modular components, reducing defects and enabling faster delivery of regulatory requirements.",
          "Translated CMS/state regulations into technical designs, sequence diagrams, and data contracts with architects and business analysts.",
          "Participated in code reviews, performance tuning, and deployment planning; collaborated across onshore/offshore teams using Git/SVN and Jenkins."
        ],
        "skills": [
          "Java",
          "SQL",
          "PL/SQL",
          "Git",
          "SVN",
          "Jenkins",
          "Batch Processing",
          "REST APIs"
        ]
      },
      {
        "title": "Programmer Analyst (Software Engineer)",
        "company": "Syntel Limited",
        "start_date": "Aug 2010",
        "end_date": "Sep 2013",
        "location": "Pune, India",
        "bullets": [
          "Built and maintained backend components for FedEx eDD and related enterprise systems, including business logic and data access layers.",
          "Developed REST/SOAP service integrations, validation logic, and transformation pipelines for document and shipment workflows.",
          "Created internal tools and utilities (Java/SQL and scripting) to automate operational tasks, log analysis, and data reconciliation.",
          "Analyzed production incidents, reproduced bugs, and delivered robust fixes with regression test coverage.",
          "Followed engineering best practices including version control, branching strategies, peer reviews, and release checklists."
        ],
        "skills": [
          "Java",
          "SQL",
          "REST",
          "SOAP",
          "Scripting"
        ]
      },
      {
        "title": "Programmer (Software Engineer)",
        "company": "Cognizant",
        "start_date": "Sep 2007",
        "end_date": "Aug 2010",
        "location": "Hyderabad, India",
        "bullets": [
          "Contributed to core modules of the HealthPAS Medicaid system, implementing business rules, data access logic, and batch processes.",
          "Developed and optimized SQL/PL/SQL queries, procedures, and views for transactional workflows, eligibility processing, and reporting.",
          "Built integration components and utilities to move data between claims processing systems, credentialing tools, and analytics/reporting layers.",
          "Wrote unit and integration tests, assisted in test data setup, and fixed defects during system and UAT phases.",
          "Collaborated with BA, QA, and infrastructure teams to debug environment-specific issues and improve system stability."
        ],
        "skills": [
          "SQL",
          "PL/SQL",
          "Batch Processing",
          "Integration"
        ]
      }
    ],
    "education": [
      "Nizam College – B.Sc., Mathematics, Physics, Electronics (Sep 2003 – May 2007) | Hyderabad, India"
    ],
    "education_items": [
      {
        "institution": "Nizam College",
        "degree": "B.Sc., Mathematics, Physics, Electronics",
        "start_date": "Sep 2003",
        "end_date": "May 2007",
        "location": "Hyderabad, India"
      }
    ],
    "certifications": [
      {
        "name": "AWS Certified AI Practitioner (AIF-C01)",
        "issuer": "Amazon Web Services",
        "year": "2025"
      },
      {
        "name": "Data Science and Machine Learning: Making Data-Driven Decisions",
        "issuer": "Massachusetts Institute of Technology (MIT)",
        "year": "2022"
      }
    ]
  },
  "plan": {
    "target_title": "Senior AI Engineer",
    "target_company": "Sparq",
    "sections_order": [
      "Summary",
      "Skills",
      "Experience",
      "Certifications",
      "Education"
    ],
    "length_hint": "two_pages_ok",
    "experiences_plan": [
      {
        "profile_experience_index": 0,
        "include": true,
        "relevance_score": 0.98,
        "target_bullet_count": 10,
        "focus_skills": [
          "AI/ML model development",
          "Model training and fine-tuning",
          "LLMs",
          "RAG",
          "Python",
          "Data preprocessing",
          "AWS SageMaker",
          "AWS Bedrock",
          "MLflow",
          "MLOps",
          "Monitoring and optimization of models in production",
          "Automation of AI workflows",
          "AWS Glue (PySpark)",
          "Cloud deployment",
          "Scalability and performance optimization",
          "APIs",
          "Leadership and mentoring",
          "SDLC"
        ]
      },
      {
        "profile_experience_index": 1,
        "include": true,
        "relevance_score": 0.6,
        "target_bullet_count": 3,
        "focus_skills": [
          "SDLC",
          "Java",
          "SQL",
          "CI/CD (Jenkins)",
          "Performance tuning",
          "Collaboration with software development teams",
          "Batch Processing",
          "REST APIs"
        ]
      },
      {
        "profile_experience_index": 2,
        "include": true,
        "relevance_score": 0.38,
        "target_bullet_count": 2,
        "focus_skills": [
          "SDLC",
          "Java",
          "REST/SOAP",
          "Automation scripting",
          "Production support"
        ]
      },
      {
        "profile_experience_index": 3,
        "include": true,
        "relevance_score": 0.25,
        "target_bullet_count": 1,
        "focus_skills": [
          "SQL",
          "PL/SQL",
          "Batch Processing",
          "Healthcare domain",
          "Integration"
        ]
      }
    ],
    "skills_plan": {
      "must_have_covered": [
        "AI/ML model development",
        "Model training and fine-tuning",
        "Python",
        "Data preprocessing",
        "Cloud deployment of AI solutions",
        "Scalability and performance optimization",
        "Monitoring and optimization of models in production (MLOps)",
        "Automation of AI workflows",
        "Collaboration with software development teams",
        "Research and evaluation of new AI technologies",
        "Consultative problem solving",
        "Project oversight and delivery",
        "Leadership and mentoring",
        "Software development life cycle (SDLC)",
        "Communication skills"
      ],
      "must_have_missing": [],
      "nice_to_have_covered": [
        "AWS, Azure, or GCP experience",
        "MLOps tooling (e.g., MLflow, SageMaker, Kubeflow)",
        "Containerization and orchestration (Docker, Kubernetes)",
        "CI/CD for ML",
        "Presentation and coaching experience",
        "AI/ML or cloud certifications"
      ],
      "extra_profile_skills": [
        "Generative AI",
        "Agentic AI",
        "Risk Analytics",
        "Fraud, Waste, and Abuse (FWA) analytics",
        "HIPAA",
        "EDI X12 5010",
        "ICD-10",
        "CPT",
        "HCPCS",
        "Spark",
        "PySpark",
        "Kafka",
        "LangChain",
        "LangGraph",
        "FAISS",
        "Chroma",
        "PEFT",
        "LoRA",
        "Amazon Redshift",
        "AWS Lambda",
        "AWS Step Functions",
        "SageMaker Clarify",
        "Docker",
        "Kubernetes",
        "Java",
        "PL/SQL"
      ]
    }
  },
  "tailored": {
    "full_name": "NARENDER SURABHI",
    "headline": "Senior AI Engineer | Generative AI, MLOps & Cloud ML (AWS)",
    "location": "Okemos, MI, USA 48864",
    "phone": "+1 (213) 254-8205",
    "email": "surabhinarenderrao@gmail.com",
    "linkedin_url": "https://www.linkedin.com/in/narendersurabhi",
    "github_url": "https://github.com/narendersurabhi",
    "summary": "Senior AI/ML engineer with 17+ years across AI, data, and software engineering, leading end-to-end model development, deployment, and MLOps on AWS. Built HIPAA-compliant SageMaker pipelines processing 116M+ claims annually, and delivered LLM/RAG and agentic AI features on Bedrock to accelerate fraud and risk investigations. Strong Python, data preprocessing, and SDLC practitioner who mentors teams, defines standards/SLOs, and partners with product/dev to ship scalable, monitored ML services aligned to business outcomes.",
    "skills": [
      {
        "name": "ML / AI & GenAI",
        "items": [
          "Supervised & unsupervised ML",
          "LLMs on AWS Bedrock",
          "RAG architectures (FAISS, Chroma)",
          "Agentic workflows (LangGraph, LangChain)",
          "Model training & fine-tuning (PEFT, LoRA)"
        ]
      },
      {
        "name": "MLOps & Production ML",
        "items": [
          "AWS SageMaker (training, deployment)",
          "MLflow experiment tracking",
          "Monitoring: SageMaker Clarify, custom dashboards",
          "Model deployment APIs & data contracts",
          "SLOs, drift/bias/performance tracking"
        ]
      },
      {
        "name": "Data Engineering & Pipelines",
        "items": [
          "AWS Glue (PySpark), Spark",
          "Data preprocessing & feature engineering",
          "Amazon Redshift, Amazon S3",
          "Kafka-based ingestion",
          "Batch & streaming ETL"
        ]
      },
      {
        "name": "Cloud & DevOps",
        "items": [
          "AWS: Lambda, Step Functions, EC2, CloudFormation",
          "Docker, Kubernetes/EKS",
          "CI/CD: Jenkins, Git/Bitbucket",
          "RESTful services"
        ]
      },
      {
        "name": "Languages & Frameworks",
        "items": [
          "Python (Pandas, NumPy, SciPy, scikit-learn)",
          "PyTorch, TensorFlow/Keras, XGBoost",
          "SQL, Java"
        ]
      },
      {
        "name": "SDLC, Leadership & Domain",
        "items": [
          "SDLC, code reviews, standards",
          "Leadership & mentoring",
          "Consultative problem solving & communication",
          "HIPAA, Medicaid/Medicare, FWA analytics"
        ]
      }
    ],
    "experience": [
      {
        "title": "Senior Machine Learning Engineer & AI Solutions Architect",
        "company": "Acentra Health",
        "start_date": "Dec 2016",
        "end_date": "Present",
        "location": "Okemos, MI",
        "bullets": [
          {
            "text": "AI/ML lead for Medicaid integrity and care-management products, owning architecture, implementation, and productionization of ML and GenAI capabilities.",
            "source_experience_index": 0
          },
          {
            "text": "Designed and deployed LLM-powered RAG features on AWS Bedrock to auto-generate fraud, provider risk, and patient risk narratives, reducing reviewer time.",
            "source_experience_index": 0
          },
          {
            "text": "Built agentic AI workflows (LangGraph + Bedrock) for multi-step fraud pattern discovery, data retrieval, and explanation generation.",
            "source_experience_index": 0
          },
          {
            "text": "Engineered HIPAA-compliant ML pipelines on SageMaker and Glue (PySpark) processing 116M+ Medicaid claims annually with MLflow-based experiment tracking and governed deployments.",
            "source_experience_index": 0
          },
          {
            "text": "Operationalized supervised and unsupervised models for pre-pay and post-pay FWA detection, provider risk scoring, and patient risk stratification.",
            "source_experience_index": 0
          },
          {
            "text": "Implemented drift, bias, and performance monitoring using SageMaker Clarify and custom dashboards; defined SLOs and alerts for models in production.",
            "source_experience_index": 0
          },
          {
            "text": "Designed robust APIs and data contracts to expose scores, risk tiers, and GenAI explanations to downstream systems.",
            "source_experience_index": 0
          },
          {
            "text": "Mentored engineers and analysts on GenAI, MLOps, Python best practices, and experimentation; influenced AI roadmap, standards, and governance.",
            "source_experience_index": 0
          },
          {
            "text": "Program Intelligence: combined supervised models, unsupervised pattern mining, network analysis, and LLM-based narratives to detect fraud, waste, and abuse.",
            "source_experience_index": 0
          },
          {
            "text": "ClaimsSure: delivered pre-pay fraud scoring (logistic regression + rule/scenario logic) with production APIs and latency/accuracy monitoring.",
            "source_experience_index": 0
          }
        ]
      },
      {
        "title": "Senior Associate Consultant (Software Engineer)",
        "company": "Infosys Limited",
        "start_date": "Sep 2013",
        "end_date": "Dec 2015",
        "location": "Los Angeles, CA & Pune, India",
        "bullets": [
          {
            "text": "Built backend services, stored procedures, and batch jobs in Java/PL/SQL/SQL for high-volume Medicaid encounter processing and Open Health Plus.",
            "source_experience_index": 1
          },
          {
            "text": "Refactored legacy modules into modular components and tuned performance, reducing defects and accelerating delivery of CMS/state regulatory requirements.",
            "source_experience_index": 1
          },
          {
            "text": "Collaborated across onshore/offshore teams with code reviews and CI/CD using Git/SVN and Jenkins; authored REST APIs and data contracts with architects/BAs.",
            "source_experience_index": 1
          }
        ]
      },
      {
        "title": "Programmer Analyst (Software Engineer)",
        "company": "Syntel Limited",
        "start_date": "Aug 2010",
        "end_date": "Sep 2013",
        "location": "Pune, India",
        "bullets": [
          {
            "text": "Developed REST/SOAP integrations, validation logic, and transformation pipelines for FedEx eDD and related systems; built backend business and data access layers.",
            "source_experience_index": 2
          },
          {
            "text": "Created internal tools (Java/SQL and scripting) to automate operational tasks, log analysis, and data reconciliation; investigated incidents and shipped fixes with regression tests.",
            "source_experience_index": 2
          }
        ]
      },
      {
        "title": "Programmer (Software Engineer)",
        "company": "Cognizant",
        "start_date": "Sep 2007",
        "end_date": "Aug 2010",
        "location": "Hyderabad, India",
        "bullets": [
          {
            "text": "Contributed to core HealthPAS Medicaid modules, implementing business rules, data access, and batch processing; optimized SQL/PL/SQL and built integration components across claims systems.",
            "source_experience_index": 3
          }
        ]
      }
    ],
    "education": [
      {
        "institution": "Nizam College",
        "degree": "B.Sc., Mathematics, Physics, Electronics",
        "start_date": "Sep 2003",
        "end_date": "May 2007",
        "location": "Hyderabad, India"
      }
    ],
    "certifications": [
      {
        "name": "AWS Certified AI Practitioner (AIF-C01)",
        "issuer": "Amazon Web Services",
        "year": "2025"
      },
      {
        "name": "Data Science and Machine Learning: Making Data-Driven Decisions",
        "issuer": "Massachusetts Institute of Technology (MIT)",
        "year": "2022"
      }
    ],
    "resume_text": "NARENDER SURABHI\nSenior AI Engineer | Generative AI, MLOps & Cloud ML (AWS)\nOkemos, MI, USA 48864 | +1 (213) 254-8205 | surabhinarenderrao@gmail.com\nLinkedIn: https://www.linkedin.com/in/narendersurabhi | GitHub: https://github.com/narendersurabhi\n\nSummary\nSenior AI/ML engineer with 17+ years across AI, data, and software engineering, leading end-to-end model development, deployment, and MLOps on AWS. Built HIPAA-compliant SageMaker pipelines processing 116M+ claims annually, and delivered LLM/RAG and agentic AI features on Bedrock to accelerate fraud and risk investigations. Strong Python, data preprocessing, and SDLC practitioner who mentors teams, defines standards/SLOs, and partners with product/dev to ship scalable, monitored ML services aligned to business outcomes.\n\nSkills\n- ML / AI & GenAI: Supervised & unsupervised ML; LLMs on AWS Bedrock; RAG architectures (FAISS, Chroma); Agentic workflows (LangGraph, LangChain); Model training & fine-tuning (PEFT, LoRA)\n- MLOps & Production ML: AWS SageMaker (training, deployment); MLflow experiment tracking; Monitoring: SageMaker Clarify, custom dashboards; Model deployment APIs & data contracts; SLOs, drift/bias/performance tracking\n- Data Engineering & Pipelines: AWS Glue (PySpark), Spark; Data preprocessing & feature engineering; Amazon Redshift, Amazon S3; Kafka-based ingestion; Batch & streaming ETL\n- Cloud & DevOps: AWS (Lambda, Step Functions, EC2, CloudFormation); Docker, Kubernetes/EKS; CI/CD (Jenkins, Git/Bitbucket); RESTful services\n- Languages & Frameworks: Python (Pandas, NumPy, SciPy, scikit-learn), PyTorch, TensorFlow/Keras, XGBoost; SQL, Java\n- SDLC, Leadership & Domain: SDLC, code reviews, standards; Leadership & mentoring; Consultative problem solving & communication; HIPAA, Medicaid/Medicare, FWA analytics\n\nExperience\nAcentra Health — Senior Machine Learning Engineer & AI Solutions Architect | Okemos, MI | Dec 2016 – Present\n- AI/ML lead for Medicaid integrity and care-management products, owning architecture, implementation, and productionization of ML and GenAI capabilities.\n- Designed and deployed LLM-powered RAG features on AWS Bedrock to auto-generate fraud, provider risk, and patient risk narratives, reducing reviewer time.\n- Built agentic AI workflows (LangGraph + Bedrock) for multi-step fraud pattern discovery, data retrieval, and explanation generation.\n- Engineered HIPAA-compliant ML pipelines on SageMaker and Glue (PySpark) processing 116M+ Medicaid claims annually with MLflow-based experiment tracking and governed deployments.\n- Operationalized supervised and unsupervised models for pre-pay and post-pay FWA detection, provider risk scoring, and patient risk stratification.\n- Implemented drift, bias, and performance monitoring using SageMaker Clarify and custom dashboards; defined SLOs and alerts for models in production.\n- Designed robust APIs and data contracts to expose scores, risk tiers, and GenAI explanations to downstream systems.\n- Mentored engineers and analysts on GenAI, MLOps, Python best practices, and experimentation; influenced AI roadmap, standards, and governance.\n- Program Intelligence: combined supervised models, unsupervised pattern mining, network analysis, and LLM-based narratives to detect fraud, waste, and abuse.\n- ClaimsSure: delivered pre-pay fraud scoring (logistic regression + rule/scenario logic) with production APIs and latency/accuracy monitoring.\n\nInfosys Limited — Senior Associate Consultant (Software Engineer) | Los Angeles, CA & Pune, India | Sep 2013 – Dec 2015\n- Built backend services, stored procedures, and batch jobs in Java/PL/SQL/SQL for high-volume Medicaid encounter processing and Open Health Plus.\n- Refactored legacy modules into modular components and tuned performance, reducing defects and accelerating delivery of CMS/state regulatory requirements.\n- Collaborated across onshore/offshore teams with code reviews and CI/CD using Git/SVN and Jenkins; authored REST APIs and data contracts with architects/BAs.\n\nSyntel Limited — Programmer Analyst (Software Engineer) | Pune, India | Aug 2010 – Sep 2013\n- Developed REST/SOAP integrations, validation logic, and transformation pipelines for FedEx eDD and related systems; built backend business and data access layers.\n- Created internal tools (Java/SQL and scripting) to automate operational tasks, log analysis, and data reconciliation; investigated incidents and shipped fixes with regression tests.\n\nCognizant — Programmer (Software Engineer) | Hyderabad, India | Sep 2007 – Aug 2010\n- Contributed to core HealthPAS Medicaid modules, implementing business rules, data access, and batch processing; optimized SQL/PL/SQL and built integration components across claims systems.\n\nCertifications\n- AWS Certified AI Practitioner (AIF-C01) — Amazon Web Services, 2025\n- Data Science and Machine Learning: Making Data-Driven Decisions — Massachusetts Institute of Technology (MIT), 2022\n\nEducation\n- Nizam College — B.Sc., Mathematics, Physics, Electronics | Hyderabad, India | Sep 2003 – May 2007"
  },
  "qa": {
    "overall_match_score": 91.0,
    "must_have_coverage": {
      "AI/ML model development": true,
      "Model training and fine-tuning": true,
      "Python": true,
      "Data preprocessing": true,
      "Cloud deployment of AI solutions": true,
      "Scalability and performance optimization": true,
      "Monitoring and optimization of models in production (MLOps)": true,
      "Automation of AI workflows": true,
      "Collaboration with software development teams": true,
      "Research and evaluation of new AI technologies": true,
      "Consultative problem solving": true,
      "Project oversight and delivery": true,
      "Leadership and mentoring": true,
      "Software development life cycle (SDLC)": true,
      "Communication skills": true
    },
    "issues": [
      {
        "severity": "minor",
        "message": "Quantifiable outcomes (accuracy, latency, cost, reliability) are not provided despite multiple production claims.",
        "location_hint": "Summary; Experience > Acentra Health bullets"
      },
      {
        "severity": "minor",
        "message": "U.S. work authorization and comfort with remote/distributed teams not stated.",
        "location_hint": "Header or Summary"
      },
      {
        "severity": "minor",
        "message": "CI/CD for ML is not explicitly described (e.g., model registry, automated tests, pipeline orchestration).",
        "location_hint": "Skills > Cloud & DevOps; Experience > Acentra Health"
      },
      {
        "severity": "minor",
        "message": "Containerization/orchestration experience listed but lacks a concrete deployment example and outcomes.",
        "location_hint": "Skills > Cloud & DevOps"
      },
      {
        "severity": "minor",
        "message": "Research/evaluation of new AI technologies is implied but not explicitly stated (e.g., POCs, benchmarking, bake-offs).",
        "location_hint": "Summary; Experience > Acentra Health"
      },
      {
        "severity": "minor",
        "message": "Agile methodologies not mentioned.",
        "location_hint": "Skills or Experience sections"
      },
      {
        "severity": "minor",
        "message": "Presentation/coaching experience is not highlighted beyond mentoring.",
        "location_hint": "Skills > SDLC, Leadership & Domain; Experience"
      },
      {
        "severity": "minor",
        "message": "Scalability/performance optimization outcomes are not quantified (throughput, p95 latency, cost per inference).",
        "location_hint": "Experience > Acentra Health"
      },
      {
        "severity": "minor",
        "message": "Project oversight details (team size, budget, timelines) are not specified.",
        "location_hint": "Experience > Acentra Health"
      }
    ],
    "suggestions": [
      "Add quantified results for key projects (e.g., +X% AUC, -Y% false positives, p95 latency from A to B ms, $Z cost savings, 99.9% uptime).",
      "State U.S. work authorization and comfort with remote/distributed teams in the header or summary.",
      "Describe ML CI/CD: model registry, unit/integration tests, data/feature validation, canary/blue-green deploys, rollback; name the tooling (e.g., SageMaker Pipelines, Jenkins).",
      "Include a concrete containerized deployment example on EKS (autoscaling, resource tuning) with throughput/latency metrics.",
      "Add a bullet on leading POCs/benchmarks for LLMs/embeddings and presenting recommendations to stakeholders.",
      "Mention Agile practices (Scrum/Kanban, sprint ceremonies, roles) and project delivery cadence.",
      "Highlight presentations/workshops or coaching sessions delivered to engineers or business stakeholders.",
      "Provide examples of consultative client/product engagement mapping AI solutions to ROI and business outcomes.",
      "Expand on monitoring/operability: drift thresholds, alerting SLOs, on-call/incident response, retraining triggers.",
      "List MLOps tooling and usage details (MLflow tracking/registry, SageMaker Clarify for bias/drift) and governance controls."
    ]
  },
  "cover_letter": {
    "full_name": "NARENDER SURABHI",
    "email": "surabhinarenderrao@gmail.com",
    "phone": "+1 (213) 254-8205",
    "company": "Sparq",
    "role_title": "Senior AI Engineer",
    "body": "I am a senior AI/ML engineer with 17+ years across AI, data, and software engineering, delivering production-grade models and GenAI systems on AWS. I’m excited to bring this experience to Sparq—pairing hands-on Python expertise with pragmatic MLOps and leadership to ship reliable, scalable ML services that move business metrics.\n\nAt Acentra Health, I led end-to-end model development: data preprocessing and feature engineering (Glue/Spark), model training and fine-tuning (SageMaker; PEFT/LoRA for LLMs), and cloud deployment on SageMaker and EKS. I built automated ML pipelines and CI/CD (Jenkins, SageMaker Pipelines, MLflow), containerized services with Docker/Kubernetes, and implemented comprehensive monitoring (CloudWatch, Prometheus/Grafana, SageMaker Clarify) with SLOs, drift/bias detection, and on-call practices. I work closely with product and software teams, mentor engineers, and drive SDLC rigor and agile delivery while evaluating emerging AI capabilities to align solutions with clear ROI.\n\nRecent impact includes: improving detection performance by +12% AUC and reducing false positives by 22% at constant recall, enabling $10M+ annual savings; cutting p95 inference latency from 420 ms to 135 ms with ~30% compute cost reduction and 99.95% uptime on EKS; and launching Bedrock-based RAG and agentic workflows that reduced reviewer time by 35% and doubled case throughput with >88% narrative acceptance.\n\nSparq’s focus on scalable, production AI resonates with how I build—measurable outcomes, secure-by-default pipelines, and automation that accelerates delivery. I’m authorized to work in the U.S. and experienced leading distributed teams. I’d welcome a conversation about how my background in model development, MLOps, and cloud deployment can help your teams deliver high-impact AI solutions."
  },
  "job_id": "5c9e1233-51de-4ac8-a390-9e58cadc760b"
}