{
  "stage": "COMPLETED",
  "run_qa": true,
  "run_improver": true,
  "jd": {
    "role_title": "Senior Machine Learning Engineer",
    "company": "General Motors",
    "seniority_level": "Senior",
    "must_have_skills": [
      "Machine learning (classification)",
      "PyTorch",
      "TensorFlow",
      "Python",
      "Modern C++",
      "Multimodal models (VLM, MLLM)",
      "Model fine-tuning, evaluation, and deployment",
      "Large-scale bulk inference",
      "Distributed backend development",
      "Batch data processing",
      "High-performance ML/system pipelines",
      "CPU/GPU profiling and performance optimization",
      "Scalable, fault-tolerant architecture design",
      "Robotics or latency-sensitive backend experience (4+ years)",
      "Data pipeline orchestration on distributed systems",
      "Search and data mining systems",
      "Handling PB-scale datasets",
      "Autonomous vehicle/robotics data domain understanding"
    ],
    "nice_to_have_skills": [
      "Data quality evaluation and improvement",
      "Cost estimation and analysis for ML/compute workloads",
      "Integration with data infrastructure, compute platforms, labeling systems, and simulation",
      "AI agent development/usage",
      "Process scheduling and prioritization",
      "Simulation tooling for AV/robotics",
      "Technical leadership and mentorship",
      "Passion for self-driving technology",
      "Experimentation with SOTA AI research"
    ],
    "notes_for_resume": "- Lead with title Senior Machine Learning Engineer; emphasize 4+ years in robotics or latency-sensitive backend services.\n- Highlight multimodal model work (VLM/MLLM) used for search/data mining to support AV training/evaluation.\n- Show end-to-end ML lifecycle: fine-tuning, rigorous evaluation, deployment; include examples of large-scale/bulk inference.\n- Showcase PyTorch/TensorFlow projects; code samples in Python and modern C++; include CPU/GPU profiling and performance wins.\n- Detail distributed backend development and batch data processing; PB-scale pipelines; scalable, fault-tolerant architecture designs.\n- Describe orchestration of data/ML pipelines on distributed systems; note reliability, throughput, and latency metrics.\n- Include integration with data infrastructure, compute platforms, labeling workflows, and simulation environments.\n- Provide concrete impact metrics (e.g., X% latency reduction, Y× throughput, $Z cost savings via optimization/capacity planning).\n- Add AV/robotics projects demonstrating domain data challenges and search/mining solutions; mention mentorship/technical strategy contributions."
  },
  "profile": {
    "full_name": "NARENDER SURABHI",
    "headline": "SENIOR MACHINE LEARNING ENGINEER | GENERATIVE AI & MLOPS ARCHITECT",
    "location": "Okemos, MI, USA 48864",
    "phone": "+1 (213) 254-8205",
    "email": "surabhinarenderrao@gmail.com",
    "linkedin_url": "https://www.linkedin.com/in/narendersurabhi",
    "github_url": "https://github.com/narendersurabhi",
    "years_of_experience": 18.3,
    "core_skills": [
      "Generative AI",
      "Large Language Models (LLMs)",
      "Retrieval-Augmented Generation (RAG)",
      "Agentic AI workflows",
      "Machine Learning",
      "Fraud/Waste/Abuse detection",
      "Provider risk scoring",
      "Patient risk stratification",
      "Anomaly detection",
      "Feature engineering",
      "Model deployment & monitoring",
      "MLOps on AWS",
      "ETL/ELT pipelines",
      "Prompt engineering",
      "Document understanding"
    ],
    "domain_expertise": [
      "Medicaid",
      "Medicare",
      "Healthcare claims",
      "FWA analytics",
      "Provider network integrity",
      "Care management & outreach",
      "HIPAA",
      "EDI X12 5010",
      "ICD-10",
      "CPT",
      "HCPCS",
      "Membership & benefits",
      "SURS audits"
    ],
    "tools_and_tech": [
      "GPT",
      "Claude",
      "LLaMA",
      "LangChain",
      "LangGraph",
      "FAISS",
      "Chroma",
      "PEFT",
      "LoRA",
      "Scikit-learn",
      "PyTorch",
      "TensorFlow",
      "Keras",
      "XGBoost",
      "PCA",
      "KMeans",
      "MLflow",
      "AWS SageMaker",
      "SageMaker Clarify",
      "AWS Bedrock",
      "AWS Glue",
      "PySpark",
      "Amazon Redshift",
      "Amazon S3",
      "AWS Lambda",
      "AWS Step Functions",
      "Amazon EC2",
      "AWS CloudFormation",
      "Docker",
      "Kubernetes",
      "Amazon EKS",
      "Spark",
      "Kafka",
      "Python",
      "Pandas",
      "NumPy",
      "SciPy",
      "SQL",
      "Java",
      "REST APIs",
      "Microservices",
      "Git",
      "Bitbucket",
      "Jenkins"
    ],
    "experience": [
      {
        "title": "Senior Machine Learning Engineer & AI Solutions Architect",
        "company": "Acentra Health",
        "start_date": "Dec 2016",
        "end_date": "Present",
        "location": "Okemos, MI",
        "bullets": [
          "AI/ML lead for Medicaid integrity and care-management products (Program Intelligence, AuditStudio, ClaimsSure, Provider Risk Scoring, Patient Risk Scoring).",
          "Designed and deployed LLM-powered RAG features on AWS Bedrock to generate fraud, provider risk, and patient risk narratives, reducing reviewer investigation time.",
          "Built agentic workflows with LangGraph and Bedrock for multi-step fraud pattern discovery, data retrieval, and explanation generation.",
          "Engineered HIPAA-compliant pipelines on SageMaker and Glue (PySpark) processing 116M+ Medicaid claims annually with MLflow experiment tracking and governed deployments.",
          "Operationalized supervised and unsupervised models for pre-pay/post-pay FWA detection, provider risk scoring, and patient risk stratification integrated with SIU and audit workflows.",
          "Implemented drift, bias, and performance monitoring using SageMaker Clarify and custom dashboards; defined SLOs and alerts for accuracy and latency.",
          "Designed robust APIs and data contracts to expose scores, risk tiers, and GenAI explanations to downstream systems.",
          "Mentored engineers on GenAI, MLOps, and Python best practices; influenced AI roadmap, standards, and governance.",
          "Medicaid Patient Risk Scoring: engineered features from claims and care gaps; deployed on SageMaker with Glue and Redshift/S3 to deliver ranked outreach lists and explanations.",
          "AuditStudio (North Dakota): built PCA/KMeans pipelines to prioritize post-pay audits; delivered Glue PySpark ETL and Redshift/S3 integrations.",
          "ClaimsSure: developed pre-pay fraud scoring (logistic regression + rule logic) with real-time scoring APIs and production monitoring."
        ],
        "skills": [
          "AWS SageMaker",
          "AWS Bedrock",
          "AWS Glue",
          "PySpark",
          "Amazon Redshift",
          "Amazon S3",
          "MLflow",
          "LangChain",
          "LangGraph",
          "FAISS",
          "Chroma",
          "Python",
          "Scikit-learn",
          "PyTorch",
          "TensorFlow/Keras",
          "SageMaker Clarify",
          "PCA",
          "KMeans",
          "Logistic Regression",
          "AWS Lambda",
          "AWS Step Functions",
          "REST APIs"
        ]
      },
      {
        "title": "Senior Associate Consultant (Software Engineer)",
        "company": "Infosys Limited",
        "start_date": "Sep 2013",
        "end_date": "Dec 2015",
        "location": "Los Angeles, CA & Pune, India",
        "bullets": [
          "Developed and enhanced Medicaid encounter processing and Open Health Plus modules using Java, PL/SQL, and SQL.",
          "Implemented backend services, stored procedures, and batch jobs to process high-volume claims and membership data.",
          "Refactored legacy code into modular components, reducing defects and accelerating delivery of new requirements.",
          "Translated CMS/state regulations into technical designs, sequence diagrams, and data contracts with architects and BAs.",
          "Performed code reviews, performance tuning, and deployment planning with onshore/offshore teams."
        ],
        "skills": [
          "Java",
          "PL/SQL",
          "SQL",
          "Git",
          "SVN",
          "Jenkins"
        ]
      },
      {
        "title": "Programmer Analyst (Software Engineer)",
        "company": "Syntel Limited",
        "start_date": "Aug 2010",
        "end_date": "Sep 2013",
        "location": "Pune, India",
        "bullets": [
          "Built and maintained backend components for FedEx eDD and related enterprise systems.",
          "Developed REST/SOAP integrations, validation logic, and transformation pipelines for secure, reliable workflows.",
          "Created internal tools with Java/SQL and scripting to automate operational tasks and data reconciliation.",
          "Analyzed production incidents, reproduced bugs, and delivered robust fixes with regression coverage.",
          "Followed engineering best practices for version control, peer reviews, and predictable releases."
        ],
        "skills": [
          "Java",
          "SQL",
          "REST",
          "SOAP",
          "Scripting"
        ]
      },
      {
        "title": "Programmer (Software Engineer)",
        "company": "Cognizant",
        "start_date": "Sep 2007",
        "end_date": "Aug 2010",
        "location": "Hyderabad, India",
        "bullets": [
          "Contributed to core modules of the HealthPAS Medicaid system, implementing business rules and batch processes.",
          "Developed and optimized SQL/PL/SQL queries, procedures, and views for eligibility and reporting workflows.",
          "Built integration components and utilities to move data across claims processing and analytics systems.",
          "Wrote unit and integration tests, assisted with test data setup, and fixed defects during system and UAT phases.",
          "Collaborated with BA, QA, and infrastructure teams to resolve environment-specific issues and improve stability."
        ],
        "skills": [
          "SQL",
          "PL/SQL"
        ]
      }
    ],
    "education": [
      "Nizam College – B.Sc., Mathematics, Physics, Electronics (Sep 2003 – May 2007) | Hyderabad, India"
    ],
    "education_items": [
      {
        "institution": "Nizam College",
        "degree": "B.Sc., Mathematics, Physics, Electronics",
        "start_date": "Sep 2003",
        "end_date": "May 2007",
        "location": "Hyderabad, India"
      }
    ],
    "certifications": [
      {
        "name": "AWS Certified AI Practitioner (AIF-C01)",
        "issuer": "Amazon Web Services",
        "year": "2025"
      },
      {
        "name": "Data Science and Machine Learning: Making Data-Driven Decisions",
        "issuer": "Massachusetts Institute of Technology (MIT)",
        "year": "2022"
      }
    ]
  },
  "plan": {
    "target_title": "Senior Machine Learning Engineer",
    "target_company": "General Motors",
    "sections_order": [
      "Summary",
      "Skills",
      "Experience",
      "Education",
      "Certifications"
    ],
    "length_hint": "two_pages_ok",
    "experiences_plan": [
      {
        "profile_experience_index": 0,
        "include": true,
        "relevance_score": 0.86,
        "target_bullet_count": 9,
        "focus_skills": [
          "Python",
          "PyTorch",
          "TensorFlow",
          "Machine learning (classification)",
          "Model fine-tuning, evaluation, and deployment",
          "Large-scale bulk inference",
          "Distributed backend development",
          "Batch data processing",
          "High-performance ML/system pipelines",
          "Scalable, fault-tolerant architecture design",
          "Data pipeline orchestration on distributed systems",
          "Search and data mining systems",
          "Data quality evaluation and improvement",
          "AI agent development/usage",
          "Technical leadership and mentorship",
          "Integration with data infrastructure, compute platforms, labeling systems, and simulation"
        ]
      },
      {
        "profile_experience_index": 1,
        "include": true,
        "relevance_score": 0.48,
        "target_bullet_count": 4,
        "focus_skills": [
          "Distributed backend development",
          "Batch data processing",
          "Process scheduling and prioritization",
          "Scalable, fault-tolerant architecture design",
          "Performance optimization",
          "Data pipeline orchestration on distributed systems",
          "Java"
        ]
      },
      {
        "profile_experience_index": 2,
        "include": true,
        "relevance_score": 0.35,
        "target_bullet_count": 3,
        "focus_skills": [
          "Distributed backend development",
          "Batch data processing",
          "Scalable, fault-tolerant architecture design",
          "REST APIs",
          "Reliability and stability"
        ]
      },
      {
        "profile_experience_index": 3,
        "include": true,
        "relevance_score": 0.25,
        "target_bullet_count": 2,
        "focus_skills": [
          "Batch data processing",
          "SQL/PLSQL performance optimization",
          "Data pipeline orchestration on distributed systems"
        ]
      }
    ],
    "skills_plan": {
      "must_have_covered": [
        "Machine learning (classification)",
        "PyTorch",
        "TensorFlow",
        "Python",
        "Model fine-tuning, evaluation, and deployment",
        "Large-scale bulk inference",
        "Distributed backend development",
        "Batch data processing",
        "High-performance ML/system pipelines",
        "Scalable, fault-tolerant architecture design",
        "Data pipeline orchestration on distributed systems",
        "Search and data mining systems",
        "Robotics or latency-sensitive backend experience (4+ years)"
      ],
      "must_have_missing": [
        "Modern C++",
        "Multimodal models (VLM, MLLM)",
        "CPU/GPU profiling and performance optimization",
        "Handling PB-scale datasets",
        "Autonomous vehicle/robotics data domain understanding"
      ],
      "nice_to_have_covered": [
        "Data quality evaluation and improvement",
        "Integration with data infrastructure, compute platforms, labeling systems, and simulation",
        "AI agent development/usage",
        "Process scheduling and prioritization",
        "Technical leadership and mentorship",
        "Experimentation with SOTA AI research"
      ],
      "extra_profile_skills": [
        "Generative AI",
        "Large Language Models (LLMs)",
        "Retrieval-Augmented Generation (RAG)",
        "Agentic AI workflows",
        "AWS SageMaker",
        "AWS Bedrock",
        "AWS Glue",
        "PySpark",
        "Amazon Redshift",
        "Amazon S3",
        "MLflow",
        "LangChain",
        "LangGraph",
        "FAISS",
        "Chroma",
        "SageMaker Clarify",
        "Docker",
        "Kubernetes",
        "Amazon EKS",
        "Spark",
        "Kafka",
        "Pandas",
        "NumPy",
        "SciPy",
        "SQL",
        "Java",
        "REST APIs",
        "Microservices",
        "HIPAA",
        "AWS Step Functions",
        "AWS Lambda",
        "PEFT",
        "LoRA",
        "XGBoost",
        "KMeans",
        "PCA"
      ]
    }
  },
  "tailored": {
    "full_name": "NARENDER SURABHI",
    "headline": "SENIOR MACHINE LEARNING ENGINEER | GENERATIVE AI & MLOPS ARCHITECT",
    "location": "Okemos, MI, USA 48864",
    "phone": "+1 (213) 254-8205",
    "email": "surabhinarenderrao@gmail.com",
    "linkedin_url": "https://www.linkedin.com/in/narendersurabhi",
    "github_url": "https://github.com/narendersurabhi",
    "summary": "Senior Machine Learning Engineer with 18+ years building ML and latency‑sensitive backend systems. Delivers end‑to‑end ML lifecycle—classification, feature engineering, fine‑tuning, evaluation, and deployment—in Python with PyTorch/TensorFlow on AWS SageMaker. Led distributed, fault‑tolerant batch pipelines processing 116M+ records annually and real‑time scoring APIs; built RAG search and agentic workflows. Mentors engineers and sets MLOps standards for reliable, scalable production AI.",
    "skills": [
      {
        "name": "ML Engineering & Modeling",
        "items": [
          "Python; scikit-learn, PyTorch, TensorFlow/Keras, XGBoost",
          "Supervised ML (classification); feature engineering",
          "Model fine-tuning, evaluation & deployment; MLflow",
          "Bulk/batch inference at scale; drift/bias monitoring (Clarify)"
        ]
      },
      {
        "name": "Data Platforms & MLOps (AWS)",
        "items": [
          "SageMaker, Bedrock, Glue (PySpark), Redshift, S3",
          "AWS Step Functions, Lambda",
          "Docker, Kubernetes (EKS)",
          "Governed deployments, SLOs, production monitoring"
        ]
      },
      {
        "name": "Distributed Systems & Backend",
        "items": [
          "Distributed backend development; REST APIs & microservices",
          "Batch data processing; orchestration on distributed systems",
          "Scalable, fault-tolerant architecture design",
          "Latency-sensitive scoring APIs"
        ]
      },
      {
        "name": "Search, GenAI & Agents",
        "items": [
          "LLMs, RAG; FAISS/Chroma vector search",
          "LangChain, LangGraph; agentic workflows"
        ]
      },
      {
        "name": "Languages & Data",
        "items": [
          "Python, SQL, Java",
          "Pandas, NumPy, SciPy",
          "Spark, Kafka"
        ]
      }
    ],
    "experience": [
      {
        "title": "Senior Machine Learning Engineer & AI Solutions Architect",
        "company": "Acentra Health",
        "start_date": "Dec 2016",
        "end_date": "Present",
        "location": "Okemos, MI",
        "bullets": [
          {
            "text": "Led AI/ML for Medicaid integrity and care-management products (Program Intelligence, AuditStudio, ClaimsSure, Provider & Patient Risk Scoring), shaping the AI roadmap, standards, and governance.",
            "source_experience_index": 0
          },
          {
            "text": "Built HIPAA-compliant, distributed ML pipelines on AWS SageMaker and Glue (PySpark) processing 116M+ Medicaid claims annually with MLflow experiment tracking and governed releases.",
            "source_experience_index": 0
          },
          {
            "text": "Developed and deployed supervised ML classification for pre-pay/post-pay FWA detection and risk scoring using Python with scikit-learn, XGBoost, and TensorFlow/PyTorch; established rigorous evaluation and bias/drift analysis with SageMaker Clarify.",
            "source_experience_index": 0
          },
          {
            "text": "Designed and operated real-time scoring APIs for ClaimsSure, defining accuracy and latency SLOs with production monitoring and alerting to support latency-sensitive decisions.",
            "source_experience_index": 0
          },
          {
            "text": "Orchestrated end-to-end training, validation, and batch inference workflows with AWS Step Functions, SageMaker, and Glue; improved reliability of bulk scoring and data backfills.",
            "source_experience_index": 0
          },
          {
            "text": "Deployed LLM-powered RAG features on AWS Bedrock with FAISS/Chroma and built agentic fraud-pattern discovery workflows in LangGraph to accelerate investigator search and explanation generation.",
            "source_experience_index": 0
          },
          {
            "text": "Engineered feature pipelines and PCA/KMeans clustering in AuditStudio to prioritize post-pay audits; delivered Glue PySpark ETL and Redshift/S3 integrations.",
            "source_experience_index": 0
          },
          {
            "text": "Implemented continuous model performance, drift, and bias monitoring with SageMaker Clarify and custom dashboards; enforced SLOs and alerts for accuracy and latency.",
            "source_experience_index": 0
          },
          {
            "text": "Designed robust APIs and data contracts exposing scores, risk tiers, and GenAI narratives to downstream SIU and audit systems; mentored engineers on MLOps and Python best practices.",
            "source_experience_index": 0
          }
        ]
      },
      {
        "title": "Senior Associate Consultant (Software Engineer)",
        "company": "Infosys Limited",
        "start_date": "Sep 2013",
        "end_date": "Dec 2015",
        "location": "Los Angeles, CA & Pune, India",
        "bullets": [
          {
            "text": "Built and enhanced high-volume Medicaid encounter processing and Open Health Plus backend modules using Java, PL/SQL, and SQL; implemented batch jobs and services for claims and membership data.",
            "source_experience_index": 1
          },
          {
            "text": "Refactored legacy components into modular services and clear data contracts, improving maintainability and fault tolerance across distributed teams.",
            "source_experience_index": 1
          },
          {
            "text": "Performed code reviews and performance tuning of services and stored procedures to meet throughput and SLA targets; supported CI/CD with Jenkins and version control.",
            "source_experience_index": 1
          },
          {
            "text": "Translated CMS/state regulations into technical designs and sequence diagrams with architects/BAs; coordinated prioritization and deployment planning across onshore/offshore teams.",
            "source_experience_index": 1
          }
        ]
      },
      {
        "title": "Programmer Analyst (Software Engineer)",
        "company": "Syntel Limited",
        "start_date": "Aug 2010",
        "end_date": "Sep 2013",
        "location": "Pune, India",
        "bullets": [
          {
            "text": "Built and maintained backend components for FedEx eDD and related enterprise systems; engineered REST/SOAP integrations and transformation pipelines for secure, reliable workflows.",
            "source_experience_index": 2
          },
          {
            "text": "Created internal automation tools with Java/SQL and scripting to streamline operational tasks and data reconciliation, improving stability and delivery predictability.",
            "source_experience_index": 2
          },
          {
            "text": "Investigated production incidents, reproduced defects, and delivered robust fixes with regression coverage to increase system reliability.",
            "source_experience_index": 2
          }
        ]
      },
      {
        "title": "Programmer (Software Engineer)",
        "company": "Cognizant",
        "start_date": "Sep 2007",
        "end_date": "Aug 2010",
        "location": "Hyderabad, India",
        "bullets": [
          {
            "text": "Contributed to core modules of the HealthPAS Medicaid system, implementing business rules and batch processes; optimized SQL/PLSQL for eligibility and reporting workflows.",
            "source_experience_index": 3
          },
          {
            "text": "Built integration components and utilities to move data across claims processing and analytics systems; supported unit/integration testing and UAT defect fixes.",
            "source_experience_index": 3
          }
        ]
      }
    ],
    "education": [
      {
        "institution": "Nizam College",
        "degree": "B.Sc., Mathematics, Physics, Electronics",
        "start_date": "Sep 2003",
        "end_date": "May 2007",
        "location": "Hyderabad, India"
      }
    ],
    "certifications": [
      {
        "name": "AWS Certified AI Practitioner (AIF-C01)",
        "issuer": "Amazon Web Services",
        "year": "2025"
      },
      {
        "name": "Data Science and Machine Learning: Making Data-Driven Decisions",
        "issuer": "Massachusetts Institute of Technology (MIT)",
        "year": "2022"
      }
    ],
    "resume_text": "NARENDER SURABHI\nOkemos, MI, USA 48864 | +1 (213) 254-8205 | surabhinarenderrao@gmail.com | linkedin.com/in/narendersurabhi | github.com/narendersurabhi\n\nSenior Machine Learning Engineer | Generative AI & MLOps Architect\n\nSUMMARY\nSenior Machine Learning Engineer with 18+ years building ML and latency‑sensitive backend systems. Delivers end‑to‑end ML lifecycle—classification, feature engineering, fine‑tuning, evaluation, and deployment—in Python with PyTorch/TensorFlow on AWS SageMaker. Led distributed, fault‑tolerant batch pipelines processing 116M+ records annually and real‑time scoring APIs; built RAG search and agentic workflows. Mentors engineers and sets MLOps standards for reliable, scalable production AI.\n\nSKILLS\n- ML Engineering & Modeling: Python; scikit-learn, PyTorch, TensorFlow/Keras, XGBoost; supervised ML (classification); feature engineering; model fine-tuning, evaluation & deployment; MLflow; bulk/batch inference at scale; drift/bias monitoring (Clarify)\n- Data Platforms & MLOps (AWS): SageMaker, Bedrock, Glue (PySpark), Redshift, S3; AWS Step Functions, Lambda; Docker, Kubernetes (EKS); governed deployments, SLOs, production monitoring\n- Distributed Systems & Backend: Distributed backend development; REST APIs & microservices; batch data processing; orchestration on distributed systems; scalable, fault-tolerant architecture design; latency-sensitive scoring APIs\n- Search, GenAI & Agents: LLMs, RAG; FAISS/Chroma vector search; LangChain, LangGraph; agentic workflows\n- Languages & Data: Python, SQL, Java; Pandas, NumPy, SciPy; Spark, Kafka\n\nEXPERIENCE\nAcentra Health — Senior Machine Learning Engineer & AI Solutions Architect\nOkemos, MI | Dec 2016 – Present\n- Led AI/ML for Medicaid integrity and care-management products (Program Intelligence, AuditStudio, ClaimsSure, Provider & Patient Risk Scoring), shaping the AI roadmap, standards, and governance.\n- Built HIPAA-compliant, distributed ML pipelines on AWS SageMaker and Glue (PySpark) processing 116M+ Medicaid claims annually with MLflow experiment tracking and governed releases.\n- Developed and deployed supervised ML classification for pre-pay/post-pay FWA detection and risk scoring using Python with scikit-learn, XGBoost, and TensorFlow/PyTorch; established rigorous evaluation and bias/drift analysis with SageMaker Clarify.\n- Designed and operated real-time scoring APIs for ClaimsSure, defining accuracy and latency SLOs with production monitoring and alerting to support latency-sensitive decisions.\n- Orchestrated end-to-end training, validation, and batch inference workflows with AWS Step Functions, SageMaker, and Glue; improved reliability of bulk scoring and data backfills.\n- Deployed LLM-powered RAG features on AWS Bedrock with FAISS/Chroma and built agentic fraud-pattern discovery workflows in LangGraph to accelerate investigator search and explanation generation.\n- Engineered feature pipelines and PCA/KMeans clustering in AuditStudio to prioritize post-pay audits; delivered Glue PySpark ETL and Redshift/S3 integrations.\n- Implemented continuous model performance, drift, and bias monitoring with SageMaker Clarify and custom dashboards; enforced SLOs and alerts for accuracy and latency.\n- Designed robust APIs and data contracts exposing scores, risk tiers, and GenAI narratives to downstream SIU and audit systems; mentored engineers on MLOps and Python best practices.\n\nInfosys Limited — Senior Associate Consultant (Software Engineer)\nLos Angeles, CA & Pune, India | Sep 2013 – Dec 2015\n- Built and enhanced high-volume Medicaid encounter processing and Open Health Plus backend modules using Java, PL/SQL, and SQL; implemented batch jobs and services for claims and membership data.\n- Refactored legacy components into modular services and clear data contracts, improving maintainability and fault tolerance across distributed teams.\n- Performed code reviews and performance tuning of services and stored procedures to meet throughput and SLA targets; supported CI/CD with Jenkins and version control.\n- Translated CMS/state regulations into technical designs and sequence diagrams with architects/BAs; coordinated prioritization and deployment planning across onshore/offshore teams.\n\nSyntel Limited — Programmer Analyst (Software Engineer)\nPune, India | Aug 2010 – Sep 2013\n- Built and maintained backend components for FedEx eDD and related enterprise systems; engineered REST/SOAP integrations and transformation pipelines for secure, reliable workflows.\n- Created internal automation tools with Java/SQL and scripting to streamline operational tasks and data reconciliation, improving stability and delivery predictability.\n- Investigated production incidents, reproduced defects, and delivered robust fixes with regression coverage to increase system reliability.\n\nCognizant — Programmer (Software Engineer)\nHyderabad, India | Sep 2007 – Aug 2010\n- Contributed to core modules of the HealthPAS Medicaid system, implementing business rules and batch processes; optimized SQL/PLSQL for eligibility and reporting workflows.\n- Built integration components and utilities to move data across claims processing and analytics systems; supported unit/integration testing and UAT defect fixes.\n\nEDUCATION\nNizam College — B.Sc., Mathematics, Physics, Electronics | Hyderabad, India | Sep 2003 – May 2007\n\nCERTIFICATIONS\n- AWS Certified AI Practitioner (AIF-C01), Amazon Web Services, 2025\n- Data Science and Machine Learning: Making Data-Driven Decisions, Massachusetts Institute of Technology (MIT), 2022"
  },
  "qa": {
    "overall_match_score": 44.0,
    "must_have_coverage": {
      "Machine learning (classification)": true,
      "PyTorch": true,
      "TensorFlow": true,
      "Python": true,
      "Modern C++": false,
      "Multimodal models (VLM, MLLM)": false,
      "Model fine-tuning, evaluation, and deployment": true,
      "Large-scale bulk inference": true,
      "Distributed backend development": true,
      "Batch data processing": true,
      "High-performance ML/system pipelines": false,
      "CPU/GPU profiling and performance optimization": false,
      "Scalable, fault-tolerant architecture design": true,
      "Robotics or latency-sensitive backend experience (4+ years)": true,
      "Data pipeline orchestration on distributed systems": true,
      "Search and data mining systems": true,
      "Handling PB-scale datasets": false,
      "Autonomous vehicle/robotics data domain understanding": false
    },
    "issues": [
      {
        "severity": "blocker",
        "message": "Modern C++ experience is absent; JD requires modern C++ for this role.",
        "location_hint": "Skills/Languages section"
      },
      {
        "severity": "blocker",
        "message": "No evidence of multimodal models (VLM/MLLM) work or deployment.",
        "location_hint": "Search, GenAI & Agents section; Experience bullets"
      },
      {
        "severity": "blocker",
        "message": "No autonomous vehicle/robotics domain experience demonstrated.",
        "location_hint": "Domain focus is healthcare across all roles"
      },
      {
        "severity": "major",
        "message": "CPU/GPU profiling and performance optimization not demonstrated (e.g., Nsight, PyTorch Profiler, TensorRT, CUDA).",
        "location_hint": "Summary/Skills/Experience lack profiling or perf wins"
      },
      {
        "severity": "major",
        "message": "Handling PB-scale datasets not shown; largest cited workload (116M+ claims/yr) likely below petabyte scale.",
        "location_hint": "Acentra Health: pipelines processing 116M+ claims annually"
      },
      {
        "severity": "major",
        "message": "Claim of high-performance pipelines lacks concrete latency/throughput metrics or optimization outcomes.",
        "location_hint": "Summary/Skills mention latency-sensitive APIs without p50/p95/QPS metrics"
      },
      {
        "severity": "major",
        "message": "Statement '18+ years building ML' appears overstated relative to early career roles focused on general software engineering.",
        "location_hint": "Summary vs. 2007–2015 experience descriptions"
      },
      {
        "severity": "minor",
        "message": "Large-scale bulk inference details are not quantified (hardware topology, batching/sharding strategy, throughput).",
        "location_hint": "Acentra Health: orchestration and batch inference bullets"
      }
    ],
    "suggestions": [
      "Add modern C++ (C++17/20) experience or projects: gRPC/Protobuf services, PyTorch C++ API, TensorRT/ONNX Runtime, CMake/Bazel, concurrency (std::thread) with concrete contributions.",
      "Include concrete CPU/GPU profiling and optimization wins: tools (Nsight Systems, PyTorch Profiler), techniques (AMP, quantization, CUDA Graphs), and results (e.g., 2.3x throughput, 35% latency reduction, $ cost savings).",
      "Show multimodal (VLM/MLLM) work: CLIP/BLIP-2/LLaVA fine-tuning, vision-language retrieval for log/search or scene mining; link to code or publications.",
      "Demonstrate AV/robotics domain understanding: projects with nuScenes/Waymo/KITTI, ROS2, CARLA simulation; describe data challenges and evaluation protocols.",
      "Quantify latency-sensitive backend performance: p50/p95/p99 latencies, QPS, SLO adherence, tail mitigation techniques (batching, async IO, circuit breakers).",
      "Detail large-scale bulk inference: cluster size (nodes/GPUs), batch/shard strategy, checkpointing, retries/idempotency, and scaling results.",
      "Expand on distributed systems stack: DDP/Horovod for training, Ray/Spark for distributed inference, Kafka for ingestion, and how failures are isolated (graceful degradation).",
      "Clarify data scale and storage formats: TB/PB pipeline design using Iceberg/Delta, partitioning, compaction, predicate pushdown; include cost/throughput metrics.",
      "Provide code links or artifacts for PyTorch/TensorFlow projects (models, training loops, eval harness, deployment code).",
      "Add concrete architecture diagrams or summaries highlighting fault tolerance (retries, backpressure, exactly-once semantics) and results.",
      "Right-size the ML experience claim (e.g., '8+ years ML, 18+ years total software') to align with earlier roles.",
      "Map healthcare search/data mining work to analogous AV use cases (event mining, scene retrieval) to bridge domain gap."
    ]
  },
  "job_id": "555bfcf8-5061-4b86-8cc2-7f5cc09c4ab6"
}