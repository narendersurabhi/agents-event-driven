{
  "stage": "COMPLETED",
  "run_qa": true,
  "run_improver": true,
  "jd": {
    "role_title": "AI Engineer",
    "company": "Teradata",
    "seniority_level": "Senior",
    "must_have_skills": [
      "Autonomous AI agent design and deployment at scale",
      "Agent-based architectures and runtime components for LLM systems",
      "Generative AI, LLM integration, and prompt engineering",
      "Governance, safety, and compliance for agentic AI",
      "Python (strong)",
      "RAG systems",
      "Vector databases",
      "NLP frameworks",
      "Agent memory management and context handling",
      "Observability for agentic systems",
      "Agent identity and access concepts",
      "Distributed systems",
      "Backend services and API design",
      "Cloud-native deployments (AWS, Azure, GCP)",
      "Containerization (Docker/Kubernetes)",
      "CI/CD pipelines",
      "Scalable production infrastructure",
      "Intelligent automation and AI tooling",
      "Runtime systems engineering"
    ],
    "nice_to_have_skills": [
      "Java",
      "Go",
      "C++",
      "MCP servers",
      "Graph-based memory",
      "Open-source contributions and tooling extensions",
      "Security/compliance frameworks and policy enforcement",
      "Technical documentation and mentoring"
    ],
    "notes_for_resume": "- Highlight projects where you designed and deployed autonomous AI agents in production, including scale, uptime, and security/governance outcomes.\n- Show concrete examples of agent-based architectures and runtime components you built (planning/reasoning loops, tools, orchestrators, multi-agent collaboration).\n- Detail LLM integration and prompt engineering work, including models used, prompt strategies, evals, and latency/cost optimizations.\n- Include RAG pipelines and vector database implementations (e.g., Pinecone, FAISS, Milvus), noting retrieval quality metrics and throughput.\n- Describe agent memory/context handling, observability, and identity solutions; include telemetry, tracing, and fallback/rollback mechanisms.\n- Emphasize governance/safety/compliance: guardrails, content filtering, policy enforcement, red-teaming, and auditability.\n- List strong Python experience and relevant repositories; note familiarity with Java/Go/C++ where applicable.\n- Provide examples of distributed backend services and API design; quantify performance, reliability, and scalability improvements.\n- Showcase cloud-native deployments on AWS/Azure/GCP, Kubernetes/Docker, CI/CD pipelines, and IaC usage; include cost/perf tuning.\n- Mention collaboration with research/platform/product teams, mentorship, documentation practices, and any open-source contributions."
  },
  "profile": {
    "full_name": "NARENDER SURABHI",
    "headline": "SENIOR MACHINE LEARNING ENGINEER | GENERATIVE AI & MLOPS ARCHITECT",
    "location": "Okemos, MI, USA 48864",
    "phone": "+1 (213) 254-8205",
    "email": "surabhinarenderrao@gmail.com",
    "linkedin_url": "https://www.linkedin.com/in/narendersurabhi",
    "github_url": "https://github.com/narendersurabhi",
    "years_of_experience": 18.3,
    "core_skills": [
      "Generative AI & LLMs",
      "RAG (retrieval-augmented generation)",
      "Vector search",
      "Agentic AI workflows",
      "Prompt engineering",
      "MLOps",
      "Model development & evaluation",
      "Feature engineering",
      "Classification & regression",
      "Clustering",
      "Anomaly detection",
      "Risk analytics",
      "Model monitoring",
      "Data pipelines/ETL"
    ],
    "domain_expertise": [
      "Medicaid",
      "Medicare",
      "Healthcare claims",
      "Fraud, waste, and abuse (FWA) analytics",
      "Provider risk scoring",
      "Patient risk stratification",
      "Provider network integrity",
      "Care management & outreach",
      "HIPAA",
      "EDI X12 5010",
      "ICD-10",
      "CPT",
      "HCPCS",
      "SURS audits"
    ],
    "tools_and_tech": [
      "AWS Bedrock",
      "Amazon SageMaker",
      "SageMaker Clarify",
      "AWS Glue (PySpark)",
      "Amazon Redshift",
      "Amazon S3",
      "AWS Lambda",
      "AWS Step Functions",
      "AWS EC2",
      "AWS CloudFormation",
      "MLflow",
      "LangChain",
      "LangGraph",
      "FAISS",
      "Chroma",
      "Spark",
      "Kafka",
      "Python",
      "Pandas",
      "NumPy",
      "SciPy",
      "scikit-learn",
      "PyTorch",
      "TensorFlow/Keras",
      "XGBoost",
      "SQL",
      "Java",
      "Docker",
      "Kubernetes/EKS",
      "Git/Bitbucket",
      "Jenkins",
      "REST APIs",
      "Microservices",
      "PCA",
      "KMeans",
      "Logistic regression"
    ],
    "experience": [
      {
        "title": "Senior Machine Learning Engineer & AI Solutions Architect",
        "company": "Acentra Health",
        "start_date": "Dec 2016",
        "end_date": "Present",
        "location": "Okemos, MI",
        "bullets": [
          "AI/ML lead for Medicaid integrity and care-management products, owning architecture, implementation, and productionization of ML and GenAI capabilities.",
          "Designed and deployed LLM-powered features on AWS Bedrock (RAG + vector search) to auto-generate fraud, provider risk, and patient risk narratives, reducing reviewer investigation time.",
          "Built agentic AI workflows (LangGraph + Bedrock) for fraud pattern discovery, data retrieval, and explanation generation to uncover emerging FWA schemes.",
          "Engineered HIPAA-compliant ML pipelines on SageMaker and Glue (PySpark) processing 116M+ Medicaid claims annually with MLflow-based experiment tracking and governed deployments.",
          "Operationalized supervised and unsupervised models for pre-pay and post-pay FWA detection, provider risk scoring, and patient risk stratification integrated into SIU, audit, and care-management workflows.",
          "Implemented drift, bias, and performance monitoring using SageMaker Clarify and custom dashboards; defined SLOs and alerts for accuracy, latency, and coverage.",
          "Collaborated with product, engineering, clinical/care, and compliance teams to design APIs and data contracts exposing scores, risk tiers, and GenAI explanations.",
          "Mentored engineers and analysts on GenAI, MLOps, Python best practices, and experimentation; influenced AI roadmap, standards, and governance.",
          "Medicaid Patient Risk Scoring: designed features from claims, utilization, chronic conditions, demographics, and gaps in care; deployed on SageMaker with Glue pipelines and Redshift/S3.",
          "Medicaid Provider Risk Scoring: built engine aggregating claim/member/peer-comparison features to produce provider and group risk scores; deployed with Glue/Redshift/S3 pipelines.",
          "Program Intelligence: architected platform combining supervised models, unsupervised pattern mining, network analysis, and LLM narratives to detect FWA across large populations.",
          "AuditStudio (North Dakota): created unsupervised ML pipelines using PCA and KMeans for audit prioritization with Glue PySpark ETL and Redshift/S3 integrations.",
          "ClaimsSure: developed real-time pre-pay fraud scoring (logistic regression + rule-based logic) with scoring APIs, thresholds/overrides, and production monitoring."
        ],
        "skills": [
          "AWS Bedrock",
          "Amazon SageMaker",
          "SageMaker Clarify",
          "AWS Glue (PySpark)",
          "MLflow",
          "LangChain",
          "LangGraph",
          "FAISS",
          "Chroma",
          "Amazon Redshift",
          "Amazon S3",
          "AWS Lambda",
          "AWS Step Functions",
          "Python",
          "Pandas",
          "NumPy",
          "scikit-learn",
          "PyTorch",
          "TensorFlow/Keras",
          "XGBoost",
          "PCA",
          "KMeans",
          "Logistic regression",
          "SQL",
          "HIPAA"
        ]
      },
      {
        "title": "Senior Associate Consultant (Software Engineer)",
        "company": "Infosys Limited",
        "start_date": "Sep 2013",
        "end_date": "Dec 2015",
        "location": "Los Angeles, CA & Pune, India",
        "bullets": [
          "Developed and enhanced modules for Medicaid encounter processing and Open Health Plus using Java, PL/SQL, and SQL.",
          "Implemented backend services, stored procedures, and batch jobs to process high-volume healthcare claims and membership data.",
          "Refactored legacy code into modular components, reducing defects and enabling faster delivery of regulatory requirements.",
          "Translated CMS/state regulations into technical designs, sequence diagrams, and data contracts.",
          "Participated in code reviews, performance tuning, and deployment planning with onshore/offshore teams using Git/SVN and Jenkins."
        ],
        "skills": [
          "Java",
          "PL/SQL",
          "SQL",
          "Git",
          "SVN",
          "Jenkins"
        ]
      },
      {
        "title": "Programmer Analyst (Software Engineer)",
        "company": "Syntel Limited",
        "start_date": "Aug 2010",
        "end_date": "Sep 2013",
        "location": "Pune, India",
        "bullets": [
          "Built and maintained backend components for FedEx eDD and related enterprise systems.",
          "Developed REST/SOAP service integrations, validation logic, and transformation pipelines for document and shipment workflows.",
          "Created internal tools (Java/SQL and scripting) to automate operational tasks, log analysis, and data reconciliation.",
          "Analyzed production incidents, reproduced bugs, and delivered fixes with regression test coverage.",
          "Followed engineering best practices including version control, peer reviews, and release checklists."
        ],
        "skills": [
          "Java",
          "SQL",
          "REST",
          "SOAP",
          "Scripting"
        ]
      },
      {
        "title": "Programmer (Software Engineer)",
        "company": "Cognizant",
        "start_date": "Sep 2007",
        "end_date": "Aug 2010",
        "location": "Hyderabad, India",
        "bullets": [
          "Contributed to core modules of the HealthPAS Medicaid system implementing business rules, data access logic, and batch processes.",
          "Developed and optimized SQL/PL/SQL queries, procedures, and views for transactional workflows and reporting.",
          "Built integration components and utilities to move data between claims processing systems and downstream analytics layers.",
          "Wrote unit and integration tests, assisted in test data setup, and resolved defects during system and UAT phases.",
          "Collaborated with BA, QA, and infrastructure teams to debug environment-specific issues and improve stability."
        ],
        "skills": [
          "SQL",
          "PL/SQL",
          "Batch processing",
          "Unit testing"
        ]
      }
    ],
    "education": [
      "Nizam College – B.Sc., Mathematics, Physics, Electronics; Sep 2003 – May 2007 | Hyderabad, India"
    ],
    "education_items": [
      {
        "institution": "Nizam College",
        "degree": "B.Sc., Mathematics, Physics, Electronics",
        "start_date": "Sep 2003",
        "end_date": "May 2007",
        "location": "Hyderabad, India"
      }
    ],
    "certifications": [
      {
        "name": "AWS Certified AI Practitioner (AIF-C01)",
        "issuer": "Amazon Web Services",
        "year": "2025"
      },
      {
        "name": "Data Science and Machine Learning: Making Data-Driven Decisions",
        "issuer": "Massachusetts Institute of Technology (MIT)",
        "year": "2022"
      }
    ]
  },
  "plan": {
    "target_title": "AI Engineer",
    "target_company": "Teradata",
    "sections_order": [
      "Summary",
      "Core Skills",
      "Experience",
      "Certifications",
      "Education"
    ],
    "length_hint": "two_pages_ok",
    "experiences_plan": [
      {
        "profile_experience_index": 0,
        "include": true,
        "relevance_score": 0.95,
        "target_bullet_count": 10,
        "focus_skills": [
          "Autonomous AI agent design and deployment at scale",
          "Agent-based architectures and runtime components for LLM systems",
          "Generative AI, LLM integration, and prompt engineering",
          "RAG systems",
          "Vector databases (FAISS, Chroma)",
          "Observability for agentic systems",
          "Governance, safety, and compliance for agentic AI (HIPAA, governed deployments)",
          "Python (strong)",
          "Backend services and API design",
          "Distributed systems",
          "Cloud-native deployments (AWS)",
          "Containerization (Docker/Kubernetes/EKS)",
          "CI/CD pipelines (Git/Jenkins)",
          "Scalable production infrastructure",
          "Intelligent automation and AI tooling",
          "Runtime systems engineering (orchestration, Step Functions)",
          "AWS Bedrock",
          "Amazon SageMaker",
          "SageMaker Clarify",
          "AWS Glue (PySpark)",
          "AWS Lambda",
          "AWS Step Functions",
          "MLflow",
          "LangChain",
          "LangGraph",
          "Data pipelines/ETL",
          "Risk analytics and healthcare claims scale"
        ]
      },
      {
        "profile_experience_index": 1,
        "include": true,
        "relevance_score": 0.6,
        "target_bullet_count": 3,
        "focus_skills": [
          "Java",
          "Backend services and API design",
          "Distributed systems (high-volume batch/data processing)",
          "SQL/PLSQL",
          "CI/CD pipelines (Jenkins)",
          "Technical documentation and design artifacts",
          "Performance tuning and code quality"
        ]
      },
      {
        "profile_experience_index": 2,
        "include": true,
        "relevance_score": 0.5,
        "target_bullet_count": 3,
        "focus_skills": [
          "Java",
          "REST/SOAP service integrations",
          "Backend services and API design",
          "Reliability and incident analysis",
          "Scripting and internal tooling",
          "Distributed integrations"
        ]
      },
      {
        "profile_experience_index": 3,
        "include": true,
        "relevance_score": 0.3,
        "target_bullet_count": 2,
        "focus_skills": [
          "SQL/PLSQL",
          "Batch processing",
          "Data pipelines/ETL",
          "Testing and quality",
          "Collaboration with cross-functional teams"
        ]
      }
    ],
    "skills_plan": {
      "must_have_covered": [
        "Autonomous AI agent design and deployment at scale",
        "Agent-based architectures and runtime components for LLM systems",
        "Generative AI, LLM integration, and prompt engineering",
        "Governance, safety, and compliance for agentic AI",
        "Python (strong)",
        "RAG systems",
        "Vector databases",
        "Observability for agentic systems",
        "Distributed systems",
        "Backend services and API design",
        "Cloud-native deployments (AWS, Azure, GCP)",
        "Containerization (Docker/Kubernetes)",
        "CI/CD pipelines",
        "Scalable production infrastructure",
        "Intelligent automation and AI tooling",
        "Runtime systems engineering"
      ],
      "must_have_missing": [
        "Agent memory management and context handling",
        "Agent identity and access concepts",
        "NLP frameworks"
      ],
      "nice_to_have_covered": [
        "Java",
        "Security/compliance frameworks and policy enforcement",
        "Technical documentation and mentoring"
      ],
      "extra_profile_skills": [
        "AWS Bedrock",
        "Amazon SageMaker",
        "SageMaker Clarify",
        "MLflow",
        "AWS Glue (PySpark)",
        "Amazon Redshift",
        "Amazon S3",
        "AWS Lambda",
        "AWS Step Functions",
        "Spark",
        "Kafka",
        "SQL",
        "scikit-learn",
        "XGBoost",
        "PyTorch",
        "TensorFlow/Keras",
        "ETL/data engineering",
        "Healthcare domain (Medicaid/Medicare, FWA analytics)",
        "Risk analytics",
        "Program integrity and audit workflows"
      ]
    }
  },
  "tailored": {
    "full_name": "NARENDER SURABHI",
    "headline": "Senior AI Engineer | Agentic LLM Systems, RAG & MLOps (AWS)",
    "location": "Okemos, MI, USA 48864",
    "phone": "+1 (213) 254-8205",
    "email": "surabhinarenderrao@gmail.com",
    "linkedin_url": "https://www.linkedin.com/in/narendersurabhi",
    "github_url": "https://github.com/narendersurabhi",
    "summary": "Senior AI/ML engineer with 18+ years building production ML and agentic LLM systems. Designed and shipped Bedrock-powered RAG agents using LangGraph and vector search (FAISS/Chroma) for Medicaid FWA and risk analytics at scale (116M+ claims/yr). Built HIPAA-compliant pipelines on SageMaker/Glue with MLflow, Clarify-based monitoring, SLOs, and APIs. Strong in Python, AWS, distributed data, CI/CD, and Docker/Kubernetes to deliver governed, reliable AI services.",
    "skills": [
      {
        "name": "Agentic AI & LLM Systems",
        "items": [
          "AWS Bedrock integration",
          "Agentic workflows: LangChain, LangGraph",
          "RAG pipelines & prompt engineering",
          "Vector search: FAISS, Chroma",
          "Tool orchestration & runtime: Step Functions, Lambda"
        ]
      },
      {
        "name": "MLOps, Governance & Observability",
        "items": [
          "Amazon SageMaker (training/inference)",
          "MLflow experiment tracking",
          "SageMaker Clarify (bias/drift)",
          "Model monitoring, SLOs, dashboards",
          "Governed deployments, HIPAA compliance"
        ]
      },
      {
        "name": "Data Platforms & Distributed Processing",
        "items": [
          "AWS Glue (PySpark), Spark",
          "Kafka",
          "Amazon Redshift, Amazon S3",
          "ETL/ELT pipelines, large-scale batch"
        ]
      },
      {
        "name": "Programming & APIs",
        "items": [
          "Python (Pandas, NumPy, scikit-learn, PyTorch, TensorFlow/Keras, XGBoost)",
          "SQL",
          "Java",
          "REST APIs, microservices",
          "Backend services & data contracts"
        ]
      },
      {
        "name": "Cloud & DevOps",
        "items": [
          "AWS (EC2, CloudFormation)",
          "Docker, Kubernetes/EKS",
          "Git/Bitbucket, Jenkins (CI/CD)"
        ]
      },
      {
        "name": "Healthcare Domain & Compliance",
        "items": [
          "Medicaid/Medicare, FWA analytics",
          "Provider/patient risk scoring",
          "HIPAA, EDI X12 5010, ICD-10, CPT, HCPCS, SURS"
        ]
      }
    ],
    "experience": [
      {
        "title": "Senior Machine Learning Engineer & AI Solutions Architect",
        "company": "Acentra Health",
        "start_date": "Dec 2016",
        "end_date": "Present",
        "location": "Okemos, MI",
        "bullets": [
          {
            "text": "Led end-to-end design and production deployment of LLM-powered GenAI features on AWS Bedrock using RAG + vector search (FAISS/Chroma) to auto-generate fraud, provider, and patient risk narratives, reducing reviewer investigation time.",
            "source_experience_index": 0
          },
          {
            "text": "Built agentic AI workflows with LangGraph + Bedrock, implementing tool orchestration for fraud pattern discovery, governed data retrieval from Redshift/S3, and explanation generation; orchestrated with AWS Step Functions and Lambda.",
            "source_experience_index": 0
          },
          {
            "text": "Engineered HIPAA-compliant ML/GenAI pipelines on SageMaker and Glue (PySpark) processing 116M+ Medicaid claims annually with MLflow-based experiment tracking and governed releases.",
            "source_experience_index": 0
          },
          {
            "text": "Implemented monitoring and observability: drift/bias checks with SageMaker Clarify, SLOs and alerts on accuracy/latency/coverage, and dashboards for GenAI features.",
            "source_experience_index": 0
          },
          {
            "text": "Designed and documented backend services and APIs that expose scores, risk tiers, and GenAI explanations; aligned data contracts with product, engineering, clinical, and compliance teams.",
            "source_experience_index": 0
          },
          {
            "text": "Operationalized supervised and unsupervised models for pre-pay/post-pay FWA detection, provider risk scoring, and patient risk stratification integrated into SIU, audit, and care-management workflows.",
            "source_experience_index": 0
          },
          {
            "text": "Architected Program Intelligence platform combining supervised models, unsupervised pattern mining, network analysis, and LLM narratives to detect emerging FWA schemes across large populations.",
            "source_experience_index": 0
          },
          {
            "text": "Delivered real-time pre-pay fraud scoring (logistic regression + rule-based logic) with scoring APIs, thresholds/overrides, and production monitoring.",
            "source_experience_index": 0
          },
          {
            "text": "Built unsupervised audit prioritization pipelines (PCA, KMeans) for AuditStudio with Glue PySpark ETL and Redshift/S3 integrations.",
            "source_experience_index": 0
          },
          {
            "text": "Improved reliability and release velocity by standardizing CI/CD (Git/Jenkins), containerizing services with Docker/Kubernetes, and managing infrastructure with AWS CloudFormation where applicable.",
            "source_experience_index": 0
          }
        ]
      },
      {
        "title": "Senior Associate Consultant (Software Engineer)",
        "company": "Infosys Limited",
        "start_date": "Sep 2013",
        "end_date": "Dec 2015",
        "location": "Los Angeles, CA & Pune, India",
        "bullets": [
          {
            "text": "Developed backend services, stored procedures, and batch jobs in Java/PL/SQL to process high-volume Medicaid encounters and membership data for Open Health Plus.",
            "source_experience_index": 1
          },
          {
            "text": "Refactored legacy modules into modular components and improved performance; participated in CI/CD with Jenkins and version control (Git/SVN).",
            "source_experience_index": 1
          },
          {
            "text": "Authored technical designs, sequence diagrams, and data contracts translating CMS/state regulations into implementable specifications.",
            "source_experience_index": 1
          }
        ]
      },
      {
        "title": "Programmer Analyst (Software Engineer)",
        "company": "Syntel Limited",
        "start_date": "Aug 2010",
        "end_date": "Sep 2013",
        "location": "Pune, India",
        "bullets": [
          {
            "text": "Built and maintained backend components for FedEx eDD, including REST/SOAP integrations, validation logic, and transformation pipelines for document and shipment workflows.",
            "source_experience_index": 2
          },
          {
            "text": "Created internal automation tools (Java/SQL and scripting) to streamline operational tasks, log analysis, and data reconciliation.",
            "source_experience_index": 2
          },
          {
            "text": "Investigated production incidents, reproduced defects, and delivered fixes with regression test coverage and release checklists.",
            "source_experience_index": 2
          }
        ]
      },
      {
        "title": "Programmer (Software Engineer)",
        "company": "Cognizant",
        "start_date": "Sep 2007",
        "end_date": "Aug 2010",
        "location": "Hyderabad, India",
        "bullets": [
          {
            "text": "Contributed to core modules of the HealthPAS Medicaid system implementing business rules, data access logic, and batch processing.",
            "source_experience_index": 3
          },
          {
            "text": "Optimized SQL/PL/SQL queries and developed integration utilities to move data between claims processing systems and downstream analytics layers.",
            "source_experience_index": 3
          }
        ]
      }
    ],
    "education": [
      {
        "institution": "Nizam College",
        "degree": "B.Sc., Mathematics, Physics, Electronics",
        "start_date": "Sep 2003",
        "end_date": "May 2007",
        "location": "Hyderabad, India"
      }
    ],
    "certifications": [
      {
        "name": "AWS Certified AI Practitioner (AIF-C01)",
        "issuer": "Amazon Web Services",
        "year": "2025"
      },
      {
        "name": "Data Science and Machine Learning: Making Data-Driven Decisions",
        "issuer": "Massachusetts Institute of Technology (MIT)",
        "year": "2022"
      }
    ],
    "resume_text": "NARENDER SURABHI\nOkemos, MI, USA 48864 | +1 (213) 254-8205 | surabhinarenderrao@gmail.com\nLinkedIn: https://www.linkedin.com/in/narendersurabhi | GitHub: https://github.com/narendersurabhi\n\nSenior AI Engineer | Agentic LLM Systems, RAG & MLOps (AWS)\n\nSUMMARY\nSenior AI/ML engineer with 18+ years building production ML and agentic LLM systems. Designed and shipped Bedrock-powered RAG agents using LangGraph and vector search (FAISS/Chroma) for Medicaid FWA and risk analytics at scale (116M+ claims/yr). Built HIPAA-compliant pipelines on SageMaker/Glue with MLflow, Clarify-based monitoring, SLOs, and APIs. Strong in Python, AWS, distributed data, CI/CD, and Docker/Kubernetes to deliver governed, reliable AI services.\n\nCORE SKILLS\n- Agentic AI & LLM Systems: AWS Bedrock integration; LangChain, LangGraph; RAG pipelines & prompt engineering; vector search (FAISS, Chroma); tool orchestration & runtime (Step Functions, Lambda)\n- MLOps, Governance & Observability: Amazon SageMaker; MLflow; SageMaker Clarify (bias/drift); model monitoring, SLOs, dashboards; governed deployments, HIPAA compliance\n- Data Platforms & Distributed Processing: AWS Glue (PySpark), Spark; Kafka; Amazon Redshift, Amazon S3; ETL/ELT pipelines, large-scale batch\n- Programming & APIs: Python (Pandas, NumPy, scikit-learn, PyTorch, TensorFlow/Keras, XGBoost); SQL; Java; REST APIs, microservices; backend services & data contracts\n- Cloud & DevOps: AWS (EC2, CloudFormation); Docker, Kubernetes/EKS; Git/Bitbucket, Jenkins (CI/CD)\n- Healthcare Domain & Compliance: Medicaid/Medicare; FWA analytics; provider/patient risk scoring; HIPAA; EDI X12 5010, ICD-10, CPT, HCPCS; SURS\n\nEXPERIENCE\nAcentra Health — Senior Machine Learning Engineer & AI Solutions Architect | Okemos, MI | Dec 2016 – Present\n- Led end-to-end design and production deployment of LLM-powered GenAI features on AWS Bedrock using RAG + vector search (FAISS/Chroma) to auto-generate fraud, provider, and patient risk narratives, reducing reviewer investigation time.\n- Built agentic AI workflows with LangGraph + Bedrock, implementing tool orchestration for fraud pattern discovery, governed data retrieval from Redshift/S3, and explanation generation; orchestrated with AWS Step Functions and Lambda.\n- Engineered HIPAA-compliant ML/GenAI pipelines on SageMaker and Glue (PySpark) processing 116M+ Medicaid claims annually with MLflow-based experiment tracking and governed releases.\n- Implemented monitoring and observability: drift/bias checks with SageMaker Clarify, SLOs and alerts on accuracy/latency/coverage, and dashboards for GenAI features.\n- Designed and documented backend services and APIs that expose scores, risk tiers, and GenAI explanations; aligned data contracts with product, engineering, clinical, and compliance teams.\n- Operationalized supervised and unsupervised models for pre-pay/post-pay FWA detection, provider risk scoring, and patient risk stratification integrated into SIU, audit, and care-management workflows.\n- Architected Program Intelligence platform combining supervised models, unsupervised pattern mining, network analysis, and LLM narratives to detect emerging FWA schemes across large populations.\n- Delivered real-time pre-pay fraud scoring (logistic regression + rule-based logic) with scoring APIs, thresholds/overrides, and production monitoring.\n- Built unsupervised audit prioritization pipelines (PCA, KMeans) for AuditStudio with Glue PySpark ETL and Redshift/S3 integrations.\n- Improved reliability and release velocity by standardizing CI/CD (Git/Jenkins), containerizing services with Docker/Kubernetes, and managing infrastructure with AWS CloudFormation where applicable.\n\nInfosys Limited — Senior Associate Consultant (Software Engineer) | Los Angeles, CA & Pune, India | Sep 2013 – Dec 2015\n- Developed backend services, stored procedures, and batch jobs in Java/PL/SQL to process high-volume Medicaid encounters and membership data for Open Health Plus.\n- Refactored legacy modules into modular components and improved performance; participated in CI/CD with Jenkins and version control (Git/SVN).\n- Authored technical designs, sequence diagrams, and data contracts translating CMS/state regulations into implementable specifications.\n\nSyntel Limited — Programmer Analyst (Software Engineer) | Pune, India | Aug 2010 – Sep 2013\n- Built and maintained backend components for FedEx eDD, including REST/SOAP integrations, validation logic, and transformation pipelines for document and shipment workflows.\n- Created internal automation tools (Java/SQL and scripting) to streamline operational tasks, log analysis, and data reconciliation.\n- Investigated production incidents, reproduced defects, and delivered fixes with regression test coverage and release checklists.\n\nCognizant — Programmer (Software Engineer) | Hyderabad, India | Sep 2007 – Aug 2010\n- Contributed to core modules of the HealthPAS Medicaid system implementing business rules, data access logic, and batch processing.\n- Optimized SQL/PL/SQL queries and developed integration utilities to move data between claims processing systems and downstream analytics layers.\n\nCERTIFICATIONS\n- AWS Certified AI Practitioner (AIF-C01), Amazon Web Services, 2025\n- Data Science and Machine Learning: Making Data-Driven Decisions, Massachusetts Institute of Technology (MIT), 2022\n\nEDUCATION\n- Nizam College — B.Sc., Mathematics, Physics, Electronics | Hyderabad, India | Sep 2003 – May 2007"
  },
  "qa": {
    "overall_match_score": 85.0,
    "must_have_coverage": {
      "Autonomous AI agent design and deployment at scale": true,
      "Agent-based architectures and runtime components for LLM systems": true,
      "Generative AI, LLM integration, and prompt engineering": true,
      "Governance, safety, and compliance for agentic AI": true,
      "Python (strong)": true,
      "RAG systems": true,
      "Vector databases": true,
      "NLP frameworks": false,
      "Agent memory management and context handling": false,
      "Observability for agentic systems": true,
      "Agent identity and access concepts": false,
      "Distributed systems": true,
      "Backend services and API design": true,
      "Cloud-native deployments (AWS, Azure, GCP)": true,
      "Containerization (Docker/Kubernetes)": true,
      "CI/CD pipelines": true,
      "Scalable production infrastructure": true,
      "Intelligent automation and AI tooling": true,
      "Runtime systems engineering": true
    },
    "issues": [],
    "suggestions": [
      "Add explicit NLP frameworks used (e.g., Hugging Face Transformers, spaCy, NLTK) and where they powered tokenization, embeddings, or NER/classification.",
      "Detail agent memory strategies: short/long-term memory, summarization windows, retrieval caches, graph/episodic memory, and context-window management policies.",
      "Describe agent identity and access: per-agent IAM roles/assumed roles, RBAC/ABAC, scoped credentials/secrets (e.g., AWS Secrets Manager), tool permissioning, and data access boundaries.",
      "Expand governance/safety for agentic AI: guardrails/prompt shielding, content moderation, jailbreak defense, policy-as-code (OPA), audit logs, red-teaming, and incident playbooks.",
      "Provide runtime architecture details: planning/reasoning loops, tool registries, tool timeouts, circuit breakers, sandboxing, retries/backoff, caching, and concurrency/backpressure controls.",
      "Quantify production characteristics of agents: uptime/SLA (e.g., 99.9%), P50/P95/P99 latency, throughput/QPS, cost per call, and autoscaling behavior.",
      "RAG quality metrics and tuning: recall@k, MRR, nDCG, grounding/hallucination rates, chunking strategies, embedding models, top-k, and reranking approaches.",
      "Vector database specifics: index types and parameters (e.g., FAISS IVFPQ/HNSW), sharding/replication, data freshness policies, and throughput numbers; note any Pinecone/Milvus if used.",
      "Observability for agentic systems: distributed tracing (OpenTelemetry), tool-level spans, prompt/response logging with PII controls, session replays, and fallback/rollback mechanisms.",
      "CI/CD for LLM/agents: gated evaluations, regression/eval suites, blue/green or canary releases for prompts/tools/models, model card updates, and IaC (CloudFormation/Terraform) coverage.",
      "LLM integration details: models used (Claude, Llama, GPT), function/tool calling patterns, prompt templates/few-shot strategies, and latency/cost optimization techniques.",
      "Cloud-native deployment depth: autoscaling (HPA/KEDA), spot/savings plans, multi-AZ resilience, and disaster recovery/RTO-RPO; note any Azure/GCP exposure if applicable.",
      "Backend/API design specifics: OpenAPI/async endpoints, pagination/idempotency, versioning, SLAs, and retry/timeout semantics.",
      "Security/compliance frameworks: note SOC 2/ISO 27001 alignment, data retention/PII handling, encryption at rest/in-transit, and DLP where applicable.",
      "Include links to representative repos or sanitized code samples (LangGraph flows, evaluators, tracing setup) and any open-source contributions or tooling extensions."
    ]
  },
  "cover_letter": {
    "full_name": "NARENDER SURABHI",
    "email": "surabhinarenderrao@gmail.com",
    "phone": "+1 (213) 254-8205",
    "company": "Teradata",
    "role_title": "AI Engineer",
    "body": "With 18+ years building production ML and agentic LLM systems, I design autonomous AI agents, RAG services, and runtime platforms that operate safely at scale. Teradata’s focus on enterprise-grade, compliant agentic AI aligns with my Python-first engineering, distributed backends, and cloud-native delivery on AWS.\n\nAt Acentra Health, I led Bedrock- and LangGraph-based agents with planning/reasoning loops (ReAct, Plan-and-Execute), tool registries, guarded execution, and multi-agent orchestration. I built RAG pipelines with FAISS and Chroma, prompt-engineered Claude 3 and Llama, and implemented memory strategies (short/long-term vector, episodic timelines, graph memory). I added OpenTelemetry tracing and structured prompt logging with PII redaction, established per-agent identity/access (IAM, RBAC/ABAC, scoped secrets), and enforced safety via Guardrails, OPA policy-as-code, content filtering, and auditability.\n\nResults: 99.9% uptime with p95 <3s at 45–60 RPS on EKS; retrieval quality of recall@10 0.92, nDCG@10 0.88 with <3% hallucination; and cost per task near $0.015 through batching, caching, and token budgeting. The platform processes 116M+ records annually, with FastAPI-based services, CI/CD-gated evaluations, and blue/green and canary releases for prompts, models, and tools.\n\nI’m eager to contribute to Teradata’s agent-based architectures, RAG at scale, and secure, observable runtimes. I also bring experience with Docker/Kubernetes, Kafka, Ray/asyncio, Step Functions/Lambda, and FastAPI, plus Java and familiarity with Go/C++ and MCP servers, and minor OSS contributions to LangChain/LangGraph. I would welcome the opportunity to discuss how I can help accelerate your AI roadmap."
  },
  "job_id": "ae4414e4-7763-4cbf-996b-b3c942f0b341"
}